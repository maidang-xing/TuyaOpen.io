"use strict";(self.webpackChunktuyaopen_io_website=self.webpackChunktuyaopen_io_website||[]).push([["3776"],{2589:function(e,n,t){t.r(n),t.d(n,{metadata:()=>i,default:()=>c,frontMatter:()=>o,contentTitle:()=>l,toc:()=>d,assets:()=>r});var i=JSON.parse('{"id":"applications/tuya.ai/ai-components/ai-audio-asr-impl","title":"AI Voice Interaction Implementation","description":"Terms and definitions","source":"@site/docs/applications/tuya.ai/ai-components/ai-audio-asr-impl.md","sourceDirName":"applications/tuya.ai/ai-components","slug":"/applications/tuya.ai/ai-components/ai-audio-asr-impl","permalink":"/docs/applications/tuya.ai/ai-components/ai-audio-asr-impl","draft":false,"unlisted":false,"editUrl":"https://github.com/Tuya-Community/TuyaOpen.io/edit/master/docs/applications/tuya.ai/ai-components/ai-audio-asr-impl.md","tags":[],"version":"current","frontMatter":{"title":"AI Voice Interaction Implementation"},"sidebar":"docs","previous":{"title":"Duo-Eyes Mood Robot","permalink":"/docs/applications/tuya.ai/demo-duo-eyes-mood"},"next":{"title":"Generic Demos","permalink":"/docs/examples/demo-generic-examples"}}'),a=t(4848),s=t(8453);let o={title:"AI Voice Interaction Implementation"},l,r={},d=[{value:"Terms and definitions",id:"terms-and-definitions",level:2},{value:"Features",id:"features",level:2},{value:"Functional modules",id:"functional-modules",level:2},{value:"Process",id:"process",level:2},{value:"Manually triggered single-turn dialogue",id:"manually-triggered-single-turn-dialogue",level:3},{value:"VAD-triggered free dialogue",id:"vad-triggered-free-dialogue",level:3},{value:"ASR wake-up for single-turn dialogue",id:"asr-wake-up-for-single-turn-dialogue",level:3},{value:"ASR wake-up for free dialogue",id:"asr-wake-up-for-free-dialogue",level:3},{value:"Development process",id:"development-process",level:2},{value:"Structs",id:"structs",level:3},{value:"Available operating modes:",id:"available-operating-modes",level:4},{value:"Event types",id:"event-types",level:4},{value:"Component state",id:"component-state",level:4},{value:"API description",id:"api-description",level:3},{value:"Initialize module",id:"initialize-module",level:4},{value:"Enable the audio module",id:"enable-the-audio-module",level:4},{value:"Set volume",id:"set-volume",level:4},{value:"Get volume",id:"get-volume",level:4},{value:"Manually start voice input",id:"manually-start-voice-input",level:4},{value:"Manually stop voice input",id:"manually-stop-voice-input",level:4},{value:"Wake up the module",id:"wake-up-the-module",level:4},{value:"Get module state",id:"get-module-state",level:4},{value:"Play built-in prompt tones",id:"play-built-in-prompt-tones",level:4},{value:"Development steps",id:"development-steps",level:3}];function u(e){let n={code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"terms-and-definitions",children:"Terms and definitions"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Term"}),(0,a.jsx)(n.th,{children:"Description"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"VAD"}),(0,a.jsx)(n.td,{children:"Voice activity detection, used to determine whether speech is present in an audio signal."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"ASR"}),(0,a.jsx)(n.td,{children:"Automatic speech recognition. Converts speech content into text or commands recognizable by a computer."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"PCM"}),(0,a.jsx)(n.td,{children:"Pulse code modulation. A lossless compression format that stores raw audio sample data directly."})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Opus"}),(0,a.jsx)(n.td,{children:"A lossy compression format optimized for a mix of speech and music."})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"features",children:"Features"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"ai_audio"})," component is primarily used for handling AI and audio-related operations, including audio input, output, configuration management, and creating AI sessions. Below is a detailed description of its functionalities:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Captures audio data."}),"\n",(0,a.jsx)(n.li,{children:"Play audio data."}),"\n",(0,a.jsx)(n.li,{children:"Create a cloud AI session: Send captured valid data to the cloud for ASR. The cloud replies based on the content recognized by ASR."}),"\n",(0,a.jsxs)(n.li,{children:["Preprocess captured audio data: Identify valid content before sending it to the cloud for processing, reducing the load on cloud resources.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"VAD"}),"\n",(0,a.jsx)(n.li,{children:"ASR: Perform wake word detection."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Four distinct ",(0,a.jsx)(n.strong,{children:"operating modes"})," are provided, based on different combinations of ",(0,a.jsx)(n.strong,{children:"dialogue mode"})," and ",(0,a.jsx)(n.strong,{children:"trigger method"}),".","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Dialogue mode","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Single-turn dialogue: Each trigger results in only one round of conversation (Q&A)."}),"\n",(0,a.jsx)(n.li,{children:"Free dialogue: After each trigger, N rounds of continuous conversation are possible."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Trigger methods:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Manual control: For example, hold down a button."}),"\n",(0,a.jsx)(n.li,{children:"VAD: Dialogue starts upon detecting sound (voice activity)."}),"\n",(0,a.jsx)(n.li,{children:"Local ASR wake-up detection: Dialogue starts upon detecting a specific wake word."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Available operating modes:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Manually triggered single-turn dialogue"}),"\n",(0,a.jsx)(n.li,{children:"VAD-triggered free dialogue"}),"\n",(0,a.jsx)(n.li,{children:"ASR wake-up for single-turn dialogue"}),"\n",(0,a.jsx)(n.li,{children:"ASR wake-up for free dialogue"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"functional-modules",children:"Functional modules"}),"\n",(0,a.jsx)(n.p,{children:"The component primarily consists of five functional modules:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Audio input module","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Captures audio data."}),"\n",(0,a.jsx)(n.li,{children:"Performs audio data preprocessing."}),"\n",(0,a.jsx)(n.li,{children:"Notifies of module state changes."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["AI agent module","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Creates cloud sessions."}),"\n",(0,a.jsxs)(n.li,{children:["Reports data to the cloud. ",(0,a.jsx)(n.strong,{children:"Default format: PCM (OPUS optional)."})]}),"\n",(0,a.jsxs)(n.li,{children:["Receives cloud data. ",(0,a.jsx)(n.strong,{children:"Default format: MP3, 16-bit width, 16 kHz sampling rate, mono."})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Cloud ASR processing module (Cloud ASR)","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Initiates reporting."}),"\n",(0,a.jsx)(n.li,{children:"Terminates reporting."}),"\n",(0,a.jsx)(n.li,{children:"Waits for cloud ASR results."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Audio playback module (Player)","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Plays audio data returned from the cloud."}),"\n",(0,a.jsx)(n.li,{children:"Plays built-in prompt tones."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Management module (Main)","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Serves as the component entry point."}),"\n",(0,a.jsx)(n.li,{children:"Manages the four modules listed above."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"process",children:"Process"}),"\n",(0,a.jsx)(n.h3,{id:"manually-triggered-single-turn-dialogue",children:"Manually triggered single-turn dialogue"}),"\n",(0,a.jsx)(n.p,{children:"Users can initiate a dialogue when triggered by an external condition. Each trigger results in exactly one turn of dialogue\u2014a single question-and-answer pair. For example, when a button is pressed and held, the user can provide voice input. Releasing the button signals the end of the voice input, after which the system waits for the AI's response."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'User: "Who are you?" (Triggered under an external condition, for example, a button is pressed and held)\nAI: "I am xxx."\nUser: "What\'s the weather today?" (Triggered under an external condition, for example, a button is pressed and held)\nAI: "xxxx."\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(5617).A+"",width:"1467",height:"1073"})}),"\n",(0,a.jsx)(n.h3,{id:"vad-triggered-free-dialogue",children:"VAD-triggered free dialogue"}),"\n",(0,a.jsx)(n.p,{children:"The device streams captured audio data to the VAD module for human voice detection. If voice activity is detected, a session is considered active. This enables users to speak naturally at any time, as the module will continuously stream their speech data to the cloud to initiate and maintain the session."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'User: "Who are you?"\nAI: "I am xxx."\nUser: "What\'s the weather today?"\nAI: "xxxx."\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(7001).A+"",width:"1467",height:"971"})}),"\n",(0,a.jsx)(n.h3,{id:"asr-wake-up-for-single-turn-dialogue",children:"ASR wake-up for single-turn dialogue"}),"\n",(0,a.jsx)(n.p,{children:"Before initiating a dialogue, the user must speak a wake word to activate the device. Each time the device is woken up, the user can only initiate one dialogue session. After the session concludes, the user must speak the wake word again to start a new interaction, similar to the behavior of smart speakers."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'User: "Hello, xxxx." (Wake word)\nAI: (Play prompt tone) "I\'m here."\nUser: "Who are you?"\nAI: "I am xxx."\nUser: "Hello, xxxx." (Wake word)\nAI: (Play prompt tone) "I\'m here."\nUser: "What\'s the weather today?"\nAI: "xxxx."\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(2281).A+"",width:"1533",height:"1302"})}),"\n",(0,a.jsx)(n.h3,{id:"asr-wake-up-for-free-dialogue",children:"ASR wake-up for free dialogue"}),"\n",(0,a.jsx)(n.p,{children:"After the user speaks the wake word to activate the device, they can engage in a continuous, multi-turn dialogue. Once awakened, if the device does not detect any sound for 30 seconds, it will automatically return to the wake word detection state."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'User: "Hello, xxxx." (Wake word)\nAI: (Play prompt tone) "I\'m here."\nUser: "Who are you?"\nAI: "I am xxx."\nUser: "What\'s the weather today?"\nAI: "xxxx."\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(9046).A+"",width:"1552",height:"1334"})}),"\n",(0,a.jsx)(n.h2,{id:"development-process",children:"Development process"}),"\n",(0,a.jsx)(n.h3,{id:"structs",children:"Structs"}),"\n",(0,a.jsx)(n.h4,{id:"available-operating-modes",children:"Available operating modes:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-C",children:"typedef uint8_t AI_AUDIO_WORK_MODE_E;\n#define AI_AUDIO_MODE_MANUAL_SINGLE_TALK     1 // Manually triggered single-turn dialogue\n#define AI_AUDIO_WORK_VAD_FREE_TALK          2 // VAD-triggered free dialogue\n#define AI_AUDIO_WORK_ASR_WAKEUP_SINGLE_TALK 3 // ASR wake-up for single-turn dialogue\n#define AI_AUDIO_WORK_ASR_WAKEUP_FREE_TALK   4 // ASR wake-up for free dialogue\n"})}),"\n",(0,a.jsx)(n.h4,{id:"event-types",children:"Event types"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"typedef enum {\n    AI_AUDIO_EVT_NONE,                      // No event\n    AI_AUDIO_EVT_HUMAN_ASR_TEXT,            // Returns user's speech-to-text result\n    AI_AUDIO_EVT_AI_REPLIES_TEXT_START,     // Starts streaming AI response text\n    AI_AUDIO_EVT_AI_REPLIES_TEXT_DATA,      // Streaming AI response text data\n    AI_AUDIO_EVT_AI_REPLIES_TEXT_END,       // Ends streaming AI response text\n    AI_AUDIO_EVT_AI_REPLIES_EMO,            // Returns AI emotion data\n    AI_AUDIO_EVT_ASR_WAKEUP,                // Wake word detected\n} AI_AUDIO_EVENT_E;\n\ntypedef struct {\n    char *name;\n    char *text;\n} AI_AUDIO_EMOTION_T;                       // Emotion data struct\n\n// Event notification callback\ntypedef void (*AI_AUDIO_EVT_INFORM_CB)(AI_AUDIO_EVENT_E event, uint8_t *data, uint32_t len, void *arg);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"component-state",children:"Component state"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"typedef enum {\n    AI_AUDIO_STATE_STANDBY,                 // Standby state\n    AI_AUDIO_STATE_LISTEN,                  // Listening\n    AI_AUDIO_STATE_UPLOAD,                  // Upload data to cloud\n    AI_AUDIO_STATE_AI_SPEAK,                // Play AI audio response from cloud\n    AI_AUDIO_STATE_MAX = 0xFF,             // Invalid state\n} AI_AUDIO_STATE_E;\n\n// State notification callback\ntypedef void (*AI_AUDIO_STATE_INFORM_CB)(AI_AUDIO_STATE_E state);\n"})}),"\n",(0,a.jsx)(n.h3,{id:"api-description",children:"API description"}),"\n",(0,a.jsx)(n.h4,{id:"initialize-module",children:"Initialize module"}),"\n",(0,a.jsx)(n.p,{children:"This API is mainly used to initialize AI-related services, audio devices, and other resources."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-C",children:"typedef struct {\n    AI_AUDIO_WORK_MODE_E work_mode;\n    AI_AUDIO_EVT_INFORM_CB evt_inform_cb;\n    AI_AUDIO_STATE_INFORM_CB state_inform_cb;\n} AI_AUDIO_CONFIG_T;\n\n/**\n * @brief Initializes the audio module with the provided configuration.\n * @param cfg Pointer to the configuration structure for the audio module.\n * @return OPERATE_RET - OPRT_OK if initialization is successful, otherwise an error code.\n */\nOPERATE_RET ai_audio_init(AI_AUDIO_CONFIG_T *cfg);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"enable-the-audio-module",children:"Enable the audio module"}),"\n",(0,a.jsx)(n.p,{children:"The audio module is disabled by default. You must call this API to enable it."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Sets the open state of the audio module.\n * @param is_open Boolean value indicating whether to open (true) or close (false) the audio module.\n * @return OPERATE_RET - OPRT_OK if the operation is successful, otherwise an error code.\n */\nOPERATE_RET ai_audio_set_open(bool is_open);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"set-volume",children:"Set volume"}),"\n",(0,a.jsx)(n.p,{children:"Set the volume of the microphone."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Sets the volume for the audio module.\n * @param volume The volume level to set.\n * @return OPERATE_RET - OPRT_OK if the volume is set successfully, otherwise an error code.\n */\nOPERATE_RET ai_audio_set_volume(uint8_t volume);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"get-volume",children:"Get volume"}),"\n",(0,a.jsx)(n.p,{children:"Get the current volume of the microphone."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Retrieves the current volume setting for the audio module.\n * @param None\n * @return uint8_t - The current volume level.\n */\nuint8_t ai_audio_get_volume(void);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"manually-start-voice-input",children:"Manually start voice input"}),"\n",(0,a.jsx)(n.p,{children:"After calling this API, the module enters a state ready to receive valid audio input. By default, all subsequently captured audio data will be streamed to the cloud for ASR recognition."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Manually starts a single talk session for AI audio.\n *\n * @param None\n * @return OPERATE_RET Operation result code.\n */\nOPERATE_RET ai_audio_manual_start_single_talk(void);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"manually-stop-voice-input",children:"Manually stop voice input"}),"\n",(0,a.jsx)(n.p,{children:"After calling this API, the module exits the state of receiving valid audio input. Subsequently captured audio data will no longer be sent to the cloud."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Manually stops a single talk session in the AI audio component.\n *\n * @return OPERATE_RET Returns the operation result. Typically, this indicates success or provides an error code.\n */\nOPERATE_RET ai_audio_manual_stop_single_talk(void);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"wake-up-the-module",children:"Wake up the module"}),"\n",(0,a.jsx)(n.p,{children:"After calling this API, the module enters a state ready to detect a new dialogue session (listening for valid audio input). If the module is currently engaged in a dialogue, that session will be interrupted."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"/**\n * @brief Sets the audio system to wakeup mode.\n *\n * This function configures the audio system to enable wakeup functionality,\n * allowing it to respond to wakeup events or keywords.\n *\n * @return OPERATE_RET Returns the operation result. Returns OPRT_OK on success, or an error code on failure.\n */\nOPERATE_RET ai_audio_set_wakeup(void);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"get-module-state",children:"Get module state"}),"\n",(0,a.jsx)(n.p,{children:"Get the current state of the module."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-C",children:"/**\n * @brief Retrieves the current state of the AI audio system.\n *\n * @return AI_AUDIO_STATE_E The current state of the AI audio system.\n */\nAI_AUDIO_STATE_E ai_audio_get_state(void);\n"})}),"\n",(0,a.jsx)(n.h4,{id:"play-built-in-prompt-tones",children:"Play built-in prompt tones"}),"\n",(0,a.jsx)(n.p,{children:"Play various built-in prompt tones, such as those indicating pairing status or dialogue mode."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"typedef enum {\n    AI_AUDIO_ALERT_NORMAL = 0,\n    AI_AUDIO_ALERT_POWER_ON,             /* Power-on announcement */\n    AI_AUDIO_ALERT_NOT_ACTIVE,           /* Device not activated, please perform pairing first */\n    AI_AUDIO_ALERT_NETWORK_CFG,          /* Enter pairing mode */\n    AI_AUDIO_ALERT_NETWORK_CONNECTED,    /* Network is connected successfully */\n    AI_AUDIO_ALERT_NETWORK_FAIL,         /* Network connection failed. Try again. */\n    AI_AUDIO_ALERT_NETWORK_DISCONNECT,   /* Network is disconnected */\n    AI_AUDIO_ALERT_BATTERY_LOW,          /* Low battery */\n    AI_AUDIO_ALERT_PLEASE_AGAIN,         /* Please say it again */\n    AI_AUDIO_ALERT_WAKEUP,               /* Hello, I'm here*/\n    AI_AUDIO_ALERT_LONG_KEY_TALK,        /* Press and hold the button to talk */\n    AI_AUDIO_ALERT_KEY_TALK,             /* Press the button to talk */\n    AI_AUDIO_ALERT_WAKEUP_TALK,          /* Wake up and talk */\n    AI_AUDIO_ALERT_FREE_TALK,            /* Free talk */\n} AI_AUDIO_ALERT_TYPE_E;\n\n/**\n * @brief Plays an alert sound based on the specified alert type.\n *\n * @param type - The type of alert to play, defined by the APP_ALERT_TYPE enum.\n * @return OPERATE_RET - Returns OPRT_OK if the alert sound is successfully played, otherwise returns an error code.\n */\nOPERATE_RET ai_audio_player_play_alert(AI_AUDIO_ALERT_TYPE_E type);\n"})}),"\n",(0,a.jsx)(n.h3,{id:"development-steps",children:"Development steps"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Call the module initialization API to set the operating mode and register the notification callbacks."}),"\n",(0,a.jsx)(n.li,{children:"Call the API to enable the audio module."}),"\n",(0,a.jsx)(n.li,{children:"Suppose the operating mode is set to manually triggered single-turn dialogue. In that case, you must call the manually start/stop voice input APIs to control the timing of voice data reporting. For other modes, the component handles this internally."}),"\n",(0,a.jsx)(n.li,{children:"Based on specific product requirements, you can implement appropriate handling for different events and states within the notification callbacks."}),"\n"]})]})}function c(e={}){let{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},9046:function(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/asr_free_talk-3ca0f7ddefc524ec712924f0f8877d5b.svg"},2281:function(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/asr_once_talk-4d0c25f2441fad5764c3cfd03ab4ccf3.svg"},5617:function(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/manual_once_talk-51f0b84ad47078fc13d44ab6baadbdfc.svg"},7001:function(e,n,t){t.d(n,{A:()=>i});let i=t.p+"assets/images/vad_free_talk-2aba55a8eb820cbe1e04430bea75e4e6.svg"},8453:function(e,n,t){t.d(n,{R:()=>o,x:()=>l});var i=t(6540);let a={},s=i.createContext(a);function o(e){let n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);